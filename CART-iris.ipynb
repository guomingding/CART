{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Untitled0.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "5W8KjC2Ipc9E",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "484e3712-2169-4a69-89dc-98109ff4271c"
   },
   "source": [
    "import numpy as np\n",
    "import copy as copy\n",
    "\n",
    "\n",
    "class Node(object):\n",
    "    def __init__(self):\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.parent = None\n",
    "        self.items = []  # a node in decision tree contains many data samples\n",
    "        self.feature = None  # indicate split dataset into two parts based on which feature\n",
    "        self.feature_value = None  # the feature's value\n",
    "\n",
    "    @property\n",
    "    # the prediction of a node depends on its majority's value\n",
    "    def predict(self):\n",
    "        maxCount = 0\n",
    "        for i in np.unique(self.items[1]):\n",
    "            if self.items[1].count(i) > maxCount:\n",
    "                maxCount = self.items[1].count(i)\n",
    "                maxPredict = i\n",
    "        return maxPredict\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.left == None and self.right == None:\n",
    "            return \"size:%d predict:%s\" % (len(self.items), str(self.predict))\n",
    "        else:\n",
    "            return \"feature:%s feature_value:%s\" % (self.feature, self.feature_value)\n",
    "\n",
    "    # calculate Gini index of a node\n",
    "    def get_leafEntropy(self):\n",
    "        g = 1\n",
    "        n = len(self.items[1])\n",
    "        p = {}\n",
    "        for item in self.items[1]:\n",
    "            p.setdefault(item, 0)\n",
    "            p[item] += 1\n",
    "        for v in p.values():\n",
    "            g -= (v / n) ** 2\n",
    "        return g\n",
    "\n",
    "    # calculate the number of children node\n",
    "    def get_leaf_num(self):\n",
    "        if self.left is not None and self.right is not None:\n",
    "            return self.right.get_leaf_num() + self.left.get_leaf_num()\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "\n",
    "class Dtree(object):\n",
    "    def __init__(self):\n",
    "        self.root = Node()\n",
    "\n",
    "    def __str__(self):\n",
    "        queue = [(self.root, -1)]\n",
    "        level = 0\n",
    "        res = []\n",
    "        while queue:\n",
    "            node, prelevel = queue.pop(0)\n",
    "            res.append(\"%d -> %d: %s\" % (prelevel, prelevel + 1, str(node)))\n",
    "            if node.left:\n",
    "                queue.append((node.left, prelevel + 1))\n",
    "            if node.right:\n",
    "                queue.append((node.right, prelevel + 1))\n",
    "\n",
    "            level += 1\n",
    "        return \"\\n\".join(res)\n",
    "\n",
    "    # calculate Gini index of a node based on a specified feature\n",
    "    def get_nodeEntropy(self, node):\n",
    "        ll = len(node.left.items[0])\n",
    "        lr = len(node.right.items[0])\n",
    "        return (ll * node.left.get_leafEntropy() + lr * node.right.get_leafEntropy()) / (ll + lr)\n",
    "\n",
    "    def split(self, feature, feature_value, idx, X):\n",
    "        div = [[], []]  # 对于不同类型的特征选用不同的划分方法：对于离散的，根据是否相等来划分；对于连续的，根据大于还是小于进行划分\n",
    "        # for i in idx:\n",
    "        #     if X[i][feature] == feature_value:\n",
    "        #         div[0].append(i)\n",
    "        #     else:\n",
    "        #         div[1].append(i)\n",
    "        # return div\n",
    "        for i in idx:\n",
    "            if X[i][feature] <= feature_value:\n",
    "                div[0].append(i)\n",
    "            else:\n",
    "                div[1].append(i)\n",
    "        return div\n",
    "\n",
    "    def get_G(self, idx, X, y):\n",
    "        g = 1\n",
    "        n = len(idx)\n",
    "        p = {}\n",
    "        for i in idx:\n",
    "            p.setdefault(y[i], 0)\n",
    "            p[y[i]] += 1\n",
    "        for v in p.values():\n",
    "            g -= (v / n) ** 2\n",
    "        return g\n",
    "\n",
    "    # choose a specified feature's value which has minimum Gini value\n",
    "    def get_bestFeatureValue_forAFeature(self, X, y, idx, feature, best_feature, best_feature_value, minG):\n",
    "        feature_vs = np.unique([X[i][feature] for i in idx])\n",
    "        for feature_v in feature_vs:\n",
    "            div = self.split(feature, feature_v, idx, X)\n",
    "            ll = len(div[0])\n",
    "            lr = len(div[1])\n",
    "            curG = (ll * self.get_G(div[0], X, y) + lr * self.get_G(div[1], X, y)) / (ll + lr)\n",
    "            if curG < minG:\n",
    "                minG = curG\n",
    "                best_feature = feature\n",
    "                best_feature_value = feature_v\n",
    "        return best_feature, best_feature_value, minG\n",
    "\n",
    "    # choose the best feature and its value such that has minimum Gini index\n",
    "    def get_bestFeatureAndValue(self, X, y, idx):\n",
    "        best_feature = 0\n",
    "        best_feature_value = X[0][best_feature]\n",
    "        minG = 1\n",
    "        for feature in range(len(X[0])):\n",
    "            best_feature, best_feature_value, minG = self.get_bestFeatureValue_forAFeature(X, y, idx, feature,\n",
    "                                                                                           best_feature,\n",
    "                                                                                           best_feature_value, minG)\n",
    "        return best_feature, best_feature_value\n",
    "\n",
    "    def create_Dtree(self, X, y):\n",
    "        queue = [(self.root, range(len(X)))]\n",
    "        while queue:\n",
    "            node, idx = queue.pop(0)\n",
    "            if len(np.unique([y[i] for i in idx])) == 1:\n",
    "                node.items = [copy.deepcopy(idx), [y[i] for i in idx]]\n",
    "                continue\n",
    "            best_feature, best_feature_value = self.get_bestFeatureAndValue(X, y, idx)\n",
    "\n",
    "            print(\"bestFeature: %s, bestFeatureValue: %s\" % (str(best_feature), str(best_feature_value)))\n",
    "\n",
    "            node.feature = best_feature\n",
    "            node.feature_value = best_feature_value\n",
    "            node.items = [copy.deepcopy(idx), [y[i] for i in idx]]  # 为便于剪枝时比较单节点树形式和子树形式的基尼系数，子树的标记也需要保存到根节点\n",
    "\n",
    "            div = self.split(best_feature, best_feature_value, idx, X)\n",
    "            if div[0] != []:\n",
    "                node.left = Node()\n",
    "                node.left.parent = node\n",
    "                queue.append((node.left, div[0]))\n",
    "            if div[1] != []:\n",
    "                node.right = Node()\n",
    "                node.right.parent = node\n",
    "                queue.append((node.right, div[1]))\n",
    "\n",
    "    def predict(self, xi):\n",
    "        node = self.root\n",
    "        while node.left or node.right:\n",
    "            if xi[node.feature] <= node.feature_value:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.predict\n",
    "\n",
    "    def get_min_gt(self):\n",
    "        minGt = 0\n",
    "        targetNode = None\n",
    "        queue = [(self.root)]\n",
    "        while queue:\n",
    "            node = queue.pop(0)\n",
    "            Ct = node.get_leafEntropy()  # 寻找最小的g(t)，见统计学习方法p86\n",
    "            CTt = self.get_nodeEntropy(node)\n",
    "            leafnum = node.get_leaf_num()\n",
    "            curGt = (Ct - CTt) / (leafnum - 1)\n",
    "            if minGt == 0 or curGt < minGt:\n",
    "                minGt = curGt\n",
    "                targetNode = node\n",
    "            if node.left.left and node.left.right:\n",
    "                queue.append((node.left))\n",
    "            if node.right.left and node.right.right:\n",
    "                queue.append((node.right))\n",
    "        return targetNode, minGt\n",
    "\n",
    "    def merge_subTree(self, node):\n",
    "        node.left = None\n",
    "        node.right = None\n",
    "\n",
    "\n",
    "from sklearn import datasets\n",
    "from numpy.random import choice\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "n = X.shape[0]\n",
    "\n",
    "train_size = int(0.6 * n)  # the size of training sets: 60% of the whole dataset\n",
    "cv_size = int(0.2 * n)  # the size of cross validation sets: 20% of the whole dataset\n",
    "test_size = int(0.2 * n)  # the size of testing sets: 20% of the whole dataset\n",
    "\n",
    "# choose training sets randomly\n",
    "train_rows = choice(range(n), size=train_size, replace=False)\n",
    "X_train = [X[i] for i in train_rows]\n",
    "y_train = [y[i] for i in train_rows]\n",
    "\n",
    "# choose cross validation sets randomly\n",
    "remains = [i for i in range(n) if i not in train_rows]\n",
    "cv_rows = choice(remains, size=cv_size, replace=False)\n",
    "X_cv = [X[i] for i in cv_rows]\n",
    "y_cv = [y[i] for i in cv_rows]\n",
    "\n",
    "# remaining is testing sets\n",
    "X_test = [X[i] for i in remains if i not in cv_rows]\n",
    "y_test = [y[i] for i in remains if i not in cv_rows]\n",
    "\n",
    "t1 = Dtree()\n",
    "t1.create_Dtree(X_train, y_train)\n",
    "\n",
    "predictTrue = 0\n",
    "queue = [t1]  # candidate queue of decision tree\n",
    "bestTree = t1\n",
    "maxAcc = 0\n",
    "alpha = []\n",
    "\n",
    "while queue:  # prune and cross validate\n",
    "    curTree = queue.pop(0)\n",
    "    predictTrue = 0\n",
    "    for i in range(len(X_cv)):\n",
    "        curPredict = curTree.predict(X_cv[i])\n",
    "        if curPredict == y_cv[i]:\n",
    "            predictTrue += 1\n",
    "    curAcc = predictTrue / len(y_cv)\n",
    "\n",
    "    print(\"the accuracy of cross validation\", curAcc)\n",
    "\n",
    "    # choose the tree which has best performance on cross validation tree\n",
    "    if curAcc > maxAcc:\n",
    "        bestTree = curTree\n",
    "        maxAcc = curAcc\n",
    "    if curTree.root.left and curTree.root.right:\n",
    "        nextTree = copy.deepcopy(curTree)\n",
    "        # the decision tree with the highest accuracy on the cross validation set is selected as the optimal one\n",
    "        bestNode, ai = nextTree.get_min_gt()\n",
    "\n",
    "        print(\"alpha: \", ai)\n",
    "\n",
    "        alpha.append(ai)\n",
    "        nextTree.merge_subTree(bestNode)\n",
    "        queue.append(nextTree)\n",
    "\n",
    "# apply the selected decision tree to training sets\n",
    "predictTrue = 0\n",
    "for i in range(len(X_train)):\n",
    "    curPredict = bestTree.predict(X_train[i])\n",
    "    if curPredict == y_train[i]:\n",
    "        predictTrue += 1\n",
    "acc = predictTrue / len(y_train)\n",
    "print(\"the accuracy of decision tree on training data\", acc)\n",
    "\n",
    "# apply the selected decision tree to testing sets\n",
    "predictTrue = 0\n",
    "for i in range(len(X_test)):\n",
    "    curPredict = bestTree.predict(X_test[i])\n",
    "    if curPredict == y_test[i]:\n",
    "        predictTrue += 1\n",
    "acc = predictTrue / len(y_test)\n",
    "print(\"the accuracy of decision tree on testing data\", acc)\n"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "bestFeature: 2, bestFeatureValue: 1.9\n",
      "bestFeature: 2, bestFeatureValue: 4.8\n",
      "bestFeature: 3, bestFeatureValue: 1.5\n",
      "bestFeature: 2, bestFeatureValue: 5.0\n",
      "bestFeature: 1, bestFeatureValue: 3.0\n",
      "bestFeature: 0, bestFeatureValue: 6.1\n",
      "the accuracy of cross validation 0.9666666666666667\n",
      "alpha:  0.005142332415059625\n",
      "the accuracy of cross validation 1.0\n",
      "alpha:  0.039999999999999966\n",
      "the accuracy of cross validation 1.0\n",
      "alpha:  0.15783068783068777\n",
      "the accuracy of cross validation 0.3\n",
      "the accuracy of decision tree 0.9\n"
     ]
    }
   ]
  }
 ]
}